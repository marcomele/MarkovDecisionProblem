class MDP(object):
	"""
		Model for a Markov Decision Process. Requires:

		states, a list of possible states
		actions, a list of applicable actions
		rewards, a list of reward values for each possible state
		transition, an interface transiction(state, action, rewards) -> (next state, probability)
		terminals, the list of terminal states
		discount, optional, default=0.99
		epsilon, optional, default=0.01

		Provides:
		value_iteration() -> None, performs the value iteration algorithm
		define_policy() -> None, define the decision policy based on the current values
		u, the computed values for each state after running value_iteration()
		policy, the generated decision policy generated by define_policy()
	"""
	def value_iteration(self):
		"""
			Applies the value iteration algorithm to compute the expected utility for the states in this MDP. Produces the array u containing the computed value for each state.
		"""
		for state in self.states:
			self.u[state] = 0 if self.rewards[state] is not None else None
		delta_stop = self.max_err * (1 - self.gamma) / self.gamma
		delta = delta_stop
		while delta >= delta_stop:
			prev = self.u.copy()
			delta = 0
			for state in [s for s in self.states if self.rewards[s]]:
				self.u[state] = self.rewards[state] + (0 if state in self.terminals else self.gamma * max(list(sum(prob * prev[succ] for (succ, prob) in self.transition(state, action, self.rewards)) for action in self.actions)))
				delta = max(delta, abs(prev[state] - self.u[state]))

	def define_policy(self):
		"""
			Defines a decision policy from the utility values generated by the value iteration algorithm. Produces a policy dictionary containing the decision for each state.
		"""
		self.policy = {}
		for state in self.states:
			if state in self.terminals:
				self.policy[state] = 'terminal'
			elif self.rewards[state]:
				self.policy[state] = max(self.actions, key = lambda action: sum(self.u[succ] * prob for (succ, prob) in self.transition(state, action, self.rewards)))

	def __init__(self, states, actions, rewards, transition, terminals, discount = 0.99, epsilon = 0.01):
		self.states = states
		self.actions = actions
		self.rewards = rewards
		self.transition = transition
		self.terminals = terminals
		self.gamma = discount
		self.max_err = epsilon

		self.u = {}
		self.policy = None

if __name__ == "__main__":
	def action_up(state, rewards):
		try:
			assert rewards[(state[0] - 1, state[1])] is not None
			return (state[0] - 1, state[1])
		except:
			return state
	def action_down(state, rewards):
		try:
			assert rewards[(state[0] + 1, state[1])] is not None
			return (state[0] + 1, state[1])
		except:
			return state
	def action_left(state, rewards):
		try:
			assert rewards[(state[0], state[1] - 1)] is not None
			return (state[0], state[1] - 1)
		except:
			return state
	def action_right(state, rewards):
		try:
			assert rewards[(state[0], state[1] + 1)] is not None
			return (state[0], state[1] + 1)
		except:
			return state
	def transition_function(state, action, rewards):
		"""
			Returns a list of couples (next state, prob) where prob is the probability P(next state|state, action).
		"""
		if action == "up":
			return [(action_up(state, rewards), 0.8), (action_left(state, rewards), 0.1), (action_right(state, rewards), 0.1)]
		if action == "down":
			return [(action_down(state, rewards), 0.8), (action_right(state, rewards), 0.1), (action_left(state, rewards), 0.1)]
		if action == "left":
			return [(action_left(state, rewards), 0.8), (action_down(state, rewards), 0.1), (action_up(state, rewards), 0.1)]
		if action == "right":
			return [(action_right(state, rewards), 0.8), (action_up(state, rewards), 0.1), (action_down(state, rewards), 0.1)]

	problem_states = [(x, y) for x in xrange(1, 4) for y in xrange(1, 5)]
	problem_actions = ["up", "down", "left", "right"]
	problem_rewards = {}
	for state in problem_states:
		problem_rewards[state] = 1 if state == (1, 4) else None if state == (2, 2) else -1 if state == (2, 4) else -0.04
	problem_terminals = [(1, 4), (2, 4)]
	problem = MDP(problem_states, problem_actions, problem_rewards, transition_function, problem_terminals)
	problem.value_iteration()
	problem.define_policy()

	print "Generated values"
	print "--------------------"
	print "\n".join((str(state) + ": " + (str(round(problem.u[state], 2) if problem_rewards[state] is not None else None)) for state in problem_states))
	print "\nPolicy"
	print "--------------------"
	print "\n".join((str(state) + " --> " + str(problem.policy[state] if problem.rewards[state] else None)) for state in problem_states)
